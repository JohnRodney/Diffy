#!/usr/bin/env python3
"""
Test downloaded autoencoder models on color data
Check if the models have learned to represent colors properly
"""

import json
import os
import sys

def load_test_colors():
    """Load some sample colors for testing"""
    try:
        with open('colors_a_f.json', 'r', encoding='utf-8') as f:
            colors_data = json.load(f)
        
        # Get first 10 colors for testing
        test_colors = colors_data[:10]
        print(f"üìä Loaded {len(test_colors)} test colors")
        
        for color in test_colors:
            print(f"  - {color['Name']}: {color['Hex(RGB)']}")
        
        return test_colors
        
    except FileNotFoundError:
        print("‚ùå colors_a_f.json not found - using dummy colors")
        return [
            {"Name": "Red", "Hex(RGB)": "#FF0000", "Red(RGB)": "100%", "Green(RGB)": "0%", "Blue(RGB)": "0%"},
            {"Name": "Green", "Hex(RGB)": "#00FF00", "Red(RGB)": "0%", "Green(RGB)": "100%", "Blue(RGB)": "0%"},
            {"Name": "Blue", "Hex(RGB)": "#0000FF", "Red(RGB)": "0%", "Green(RGB)": "0%", "Blue(RGB)": "100%"},
        ]

def create_simple_tokenizer(colors):
    """Create a simple tokenizer for color names"""
    vocab = {}
    
    for color in colors:
        name = color['Name']
        words = name.lower().split()
        
        for word in words:
            if word not in vocab:
                vocab[word] = len(vocab)
    
    print(f"üìù Created vocabulary with {len(vocab)} words")
    return vocab

def color_to_simple_vector(color_data, vocab, vector_length=512):
    """Convert color data to a simple vector representation"""
    try:
        # Start with basic features
        features = []
        
        # Add RGB percentages
        rgb_values = [
            float(color_data['Red(RGB)'].replace('%', '')) / 100.0,
            float(color_data['Green(RGB)'].replace('%', '')) / 100.0,
            float(color_data['Blue(RGB)'].replace('%', '')) / 100.0
        ]
        features.extend(rgb_values)
        
        # Add HSL values if available
        if 'Hue(HSL/HSV)' in color_data:
            hue = float(color_data['Hue(HSL/HSV)'].replace('¬∞', '')) / 360.0
            features.append(hue)
        
        # Add tokenized name
        name = color_data['Name'].lower()
        name_tokens = []
        for word in name.split():
            if word in vocab:
                name_tokens.append(vocab[word] / len(vocab))  # Normalize
            else:
                name_tokens.append(0.0)
        
        # Add up to 10 name tokens
        while len(name_tokens) < 10:
            name_tokens.append(0.0)
        features.extend(name_tokens[:10])
        
        # Pad to vector length
        while len(features) < vector_length:
            features.append(0.0)
        
        return features[:vector_length]
        
    except Exception as e:
        print(f"‚ùå Error converting color {color_data.get('Name', 'unknown')}: {e}")
        return [0.0] * vector_length

def test_model_structure(model_path):
    """Test loading and examining model structure"""
    print(f"\nüîç Testing model: {os.path.basename(model_path)}")
    
    try:
        import numpy as np
        
        # Load model
        model_data = np.load(model_path)
        print(f"‚úÖ Model loaded successfully")
        print(f"üìä Model contains: {list(model_data.keys())}")
        
        # Check if this looks like autoencoder weights
        weight_keys = [k for k in model_data.keys() if 'weight' in k.lower()]
        bias_keys = [k for k in model_data.keys() if 'bias' in k.lower()]
        
        print(f"üî¢ Weight layers: {len(weight_keys)}")
        print(f"üî¢ Bias layers: {len(bias_keys)}")
        
        if weight_keys:
            first_weight = model_data[weight_keys[0]]
            print(f"üìê Input size: {first_weight.shape[0]}")
            print(f"üìê First layer shape: {first_weight.shape}")
            
            # Calculate total parameters
            total_params = 0
            for key in weight_keys:
                weight = model_data[key]
                total_params += weight.size
                print(f"   {key}: {weight.shape}")
            
            for key in bias_keys:
                bias = model_data[key]
                total_params += bias.size
                print(f"   {key}: {bias.shape}")
                
            print(f"üìä Total parameters: {total_params:,}")
            
        return True
        
    except ImportError:
        print("‚ùå NumPy not available - cannot load model")
        return False
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        return False

def main():
    """Main testing function"""
    print("üß™ Color Model Testing")
    print("=" * 50)
    
    # Check if models exist
    models_dir = "downloaded_models"
    if not os.path.exists(models_dir):
        print(f"‚ùå {models_dir} directory not found")
        return
    
    # Get model files
    model_files = [f for f in os.listdir(models_dir) if f.endswith('.npz')]
    model_files.sort()
    
    if not model_files:
        print("‚ùå No model files found")
        return
    
    print(f"üìä Found {len(model_files)} models")
    
    # Test model structure
    for model_file in model_files[-3:]:  # Test last 3 models
        model_path = os.path.join(models_dir, model_file)
        test_model_structure(model_path)
    
    # Load test colors
    print("\nüé® Loading test colors...")
    test_colors = load_test_colors()
    
    # Create tokenizer
    vocab = create_simple_tokenizer(test_colors)
    
    # Convert colors to vectors
    print("\nüîÑ Converting colors to vectors...")
    for color in test_colors[:5]:  # Test first 5
        vector = color_to_simple_vector(color, vocab)
        print(f"  {color['Name']}: {len(vector)} dimensions")
        print(f"    RGB: {vector[:3]}")
        print(f"    First tokens: {vector[4:8]}")
    
    print("\n‚úÖ Model testing complete!")
    print("üéØ Models appear to be trained on color data with these vector sizes")

if __name__ == "__main__":
    main() 